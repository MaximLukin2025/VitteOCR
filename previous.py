# -*- coding: utf-8 -*-
# ---------------------------
# 0. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
# ---------------------------
!apt-get update -qq
!apt-get install -y -qq tesseract-ocr libtesseract-dev
!pip install -q torch torchvision torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118
!pip install -q einops pillow albumentations==1.4.0 pytesseract==0.3.10 editdistance opencv-python-headless timm==0.9.2 onnx onnxruntime torchinfo kenlm wget icecream torchmetrics==1.3.0

# ---------------------------
# 1. –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫
# ---------------------------
import os, random, re, string, sys, math, json, zipfile, shutil, subprocess, time, urllib.request, itertools
from pathlib import Path
from typing import List, Tuple, Dict

import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image, ImageOps, ImageFilter

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import Adam, SGD
from torchvision import transforms
from torchvision.transforms.functional import resize, to_tensor, normalize

from einops import rearrange
import timm                   # –¥–ª—è ViT backbone
import pytesseract
import editdistance
from torchmetrics.text import CharErrorRate

from icecream import ic

torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Device:", device)

# ---------------------------
# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
# ---------------------------
DATA_DIR = Path('/content/dataset')
if not DATA_DIR.exists():
    !wget -q "https://www.dropbox.com/scl/fi/mmyjbuglk8iwvybw7qmf5/dataset.zip?rlkey=v10y9pqrqobbmy0tl0zm4l0jl&st=9go362ir&dl=1" -O dataset.zip
    with zipfile.ZipFile('dataset.zip') as zf:
        zf.extractall('/content/')
    print("Dataset extracted to", DATA_DIR)

# -----------------------------------------------------------------
# 3. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
# -----------------------------------------------------------------
def show_samples(n=6):
    imgs = random.sample(list(DATA_DIR.glob('Screenshot_*.jpg')), k=n)
    plt.figure(figsize=(15,3))
    for i,p in enumerate(imgs,1):
        img = Image.open(p)
        plt.subplot(1,n,i); plt.imshow(img); plt.axis('off'); plt.title(p.name)
    plt.show()

show_samples()

# --- OCR-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞ ----------
def preprocess_pillow(img: Image.Image,
                      target_h: int = 256) -> Image.Image:
    """Gray ‚Üí CLAHE ‚Üí Binarize ‚Üí Despeckle"""
    img = img.convert('L')
    np_img = np.array(img)
    # CLAHE
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    np_img = clahe.apply(np_img)
    # adaptive threshold
    np_img = cv2.adaptiveThreshold(np_img,255,
                                   cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                   cv2.THRESH_BINARY, 31, 11)
    # median filter
    np_img = cv2.medianBlur(np_img,3)
    img = Image.fromarray(np_img)
    # resize height
    w,h = img.size
    new_w = int(w * (target_h / h))
    img = img.resize((new_w, target_h), Image.BICUBIC)
    return img

# ---------------------------------------------------------------
# 4. –î–∞—Ç–∞—Å–µ—Ç –∏ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è Self-Supervised –æ–±—É—á–µ–Ω–∏—è (SimCLR++)
# ---------------------------------------------------------------
class TextSimCLRDataset(Dataset):
    def __init__(self, paths: List[Path], out_size: Tuple[int,int]=(224,224)):
        self.paths = paths
        self.out_size = out_size
        # Albumentations pipeline —Å OCR-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è–º–∏
        import albumentations as A
        self.aug = A.Compose([
            A.RandomResizedCrop(size=(out_size[0], out_size[1]), scale=(0.7, 1.0), p=1.0),
            A.OneOf([
                A.MotionBlur(blur_limit=5, p=0.3),
                A.MedianBlur(blur_limit=5, p=0.3),
                A.GaussianBlur(blur_limit=5, p=0.3)
            ], p=0.3),
            A.GaussNoise(var_limit=(10.0,50.0), p=0.3),
            A.ImageCompression(quality_lower=50, quality_upper=100, p=0.2),
            A.RandomBrightnessContrast(p=0.3),
            A.ShiftScaleRotate(shift_limit=0.03, scale_limit=0.05,
                               rotate_limit=3, border_mode=cv2.BORDER_CONSTANT,
                               value=255, p=0.5),
        ])
        self.totensor = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
    def __len__(self): return len(self.paths)
    def __getitem__(self, idx):
        p = self.paths[idx]
        img = Image.open(p)
        img = preprocess_pillow(img).convert("RGB")  # ‚Üí 3-chan PIL
        img_np = np.array(img)
        # –¥–≤–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        im1 = self.aug(image=img_np)['image']
        im2 = self.aug(image=img_np)['image']
        if im1.ndim == 2:                       # –≤–¥—Ä—É–≥ –æ–¥–Ω–æ–∫–∞–Ω–∞–ª
          im1 = np.repeat(im1[..., None], 3, -1)
          im2 = np.repeat(im2[..., None], 3, -1)
        im1 = self.totensor(im1).float()        # 3√óH√óW
        im2 = self.totensor(im2).float()
        return im1, im2

# ---------------------------------------------------------------
# 5. Self-Supervised –º–æ–¥–µ–ª—å: ViT-Base + Projection head (SimCLR)
#    + MPLM ‚Äî Random mask –æ—Ç–¥–µ–ª—å–Ω—ã—Ö patch-—Ç–æ–∫–µ–Ω–æ–≤
# ---------------------------------------------------------------
class ViTBackbone(nn.Module):
    def __init__(self, name='vit_base_patch16_224'):
        super().__init__()
        self.backbone = timm.create_model(name, pretrained=True)
        # —É–¥–∞–ª—è–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.feature_dim = self.backbone.head.in_features
        self.backbone.reset_classifier(0)
    def forward(self,x):
        return self.backbone.forward_features(x)  # (B,N,dim)

class ProjectionHead(nn.Module):
    def __init__(self, in_dim, hid_dim=512, out_dim=128):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(in_dim, hid_dim),
            nn.BatchNorm1d(hid_dim),
            nn.ReLU(True),
            nn.Linear(hid_dim, out_dim)
        )
    def forward(self, x):
        # x: (B, N, in_dim) ‚Üí —É—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ patch
        x = x.mean(dim=1)
        return F.normalize(self.fc(x), dim=1)

class SimCLR_MPLM(nn.Module):
    def __init__(self, temperature=0.1, mask_ratio=0.25):
        super().__init__()
        self.encoder = ViTBackbone()
        self.proj = ProjectionHead(self.encoder.feature_dim)
        self.temperature = temperature
        self.mask_ratio = mask_ratio
    def mask_patches(self, x):
        # x: (B,N,D) ‚Äì –∑–∞–º–∞—Å–∫–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ patch'–∏ –Ω—É–ª—è–º–∏
        B,N,D = x.shape
        num_mask = int(N * self.mask_ratio)
        for b in range(B):
            idx = torch.randperm(N)[:num_mask]
            x[b, idx] = 0
        return x
    def forward(self, im1, im2):
        z1 = self.proj(self.mask_patches(self.encoder(im1)))
        z2 = self.proj(self.mask_patches(self.encoder(im2)))
        # InfoNCE loss
        sim = torch.matmul(z1, z2.T) / self.temperature
        labels = torch.arange(len(z1), device=z1.device)
        loss = (F.cross_entropy(sim, labels) + F.cross_entropy(sim.T, labels)) / 2
        return loss

# ---------------------------------------------------------------
# 6. –û–±—É—á–µ–Ω–∏–µ Self-Supervised (—Ñ–∞–∑–∞ 1)
# ---------------------------------------------------------------
def train_ssl(epochs=10, bs=16, lr=3e-4):
    paths = list(DATA_DIR.glob('Screenshot_*.jpg'))
    ds = TextSimCLRDataset(paths)
    loader = DataLoader(ds, batch_size=bs, shuffle=True,
                        num_workers=2, drop_last=True)
    model = SimCLR_MPLM().to(device)
    opt = Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    losses = []
    for epoch in range(epochs):
        model.train()
        ep_loss = 0
        for im1, im2 in loader:
            im1, im2 = im1.to(device), im2.to(device)
            loss = model(im1, im2)
            opt.zero_grad(); loss.backward(); opt.step()
            ep_loss += loss.item()
        ep_loss /= len(loader)
        losses.append(ep_loss)
        print(f'[SSL] Epoch {epoch+1}/{epochs}  loss: {ep_loss:.4f}')
    torch.save(model.encoder.state_dict(), 'vit_mplm_ssl.pth')
    return losses, model.encoder

ssl_losses, ssl_encoder = train_ssl(epochs=20, bs=8, lr=1e-4)

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –∫—Ä–∏–≤—É—é self-supervised loss
plt.figure(figsize=(6,4)); plt.plot(ssl_losses); plt.title('SSL loss'); plt.xlabel('epoch'); plt.ylabel('loss'); plt.show()

# ---------------------------------------------------------------
# 7. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫ (Tesseract)
# ---------------------------------------------------------------
!wget -q https://github.com/tesseract-ocr/tessdata_best/raw/main/rus.traineddata \
       -O /usr/share/tesseract-ocr/4.00/tessdata/rus.traineddata

TESSERACT_CONFIG = "--oem 1 --psm 6"  # LSTM mode, assume a single text block

def clean_text(txt: str) -> str:
    txt = txt.strip()
    txt = re.sub(r'[^0-9A-Za-z–ê-–Ø–∞-—è:%()., ]+', ' ', txt)
    txt = re.sub(r'\s{2,}', ' ', txt)
    return txt

pseudo_labels = {}
for img_path in DATA_DIR.glob('Screenshot_*.jpg'):
    img = preprocess_pillow(Image.open(img_path))
    txt = pytesseract.image_to_string(img, lang="rus", config=TESSERACT_CONFIG)
    pseudo_labels[img_path.name] = clean_text(txt)

# ==============================================================
# 8. –ß–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –º–µ—Ç–æ–∫ + –∞–ª—Ñ–∞–≤–∏—Ç
# ==============================================================

# —É–¥–∞–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≥–¥–µ Tesseract –Ω–∏—á–µ–≥–æ –Ω–µ —É–≤–∏–¥–µ–ª
pseudo_labels = {k:v for k,v in pseudo_labels.items() if v.strip()}
print(f'–ü–æ—Å–ª–µ —á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(pseudo_labels)} –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫')

ALL_CHARS = sorted(
    set(''.join(pseudo_labels.values()) +
        " .,:%()"+string.ascii_letters+string.digits)
)
char2idx = {c:i+1 for i,c in enumerate(ALL_CHARS)}   # 0=blank
idx2char = {i:c for c,i in char2idx.items()}

def encode_text(t): return [char2idx[c] for c in t if c in char2idx]
def decode_seq(seq): return ''.join(idx2char.get(i,'') for i in seq)

# ==============================================================
# 9. OCR-–¥–∞—Ç–∞—Å–µ—Ç –∏ collate_fn
# ==============================================================
class OCRDataset(Dataset):
    def __init__(self, labels, img_h=64):
        self.items = list(labels.items())
        self.img_h = img_h
        self.totensor = transforms.ToTensor()
    def __len__(self): return len(self.items)
    def __getitem__(self, idx):
        name, txt = self.items[idx]
        img = preprocess_pillow(Image.open(DATA_DIR/name), target_h=self.img_h)
        img = self.totensor(img)           # 1√óH√óW
        lbl = torch.tensor(encode_text(txt), dtype=torch.long)
        return img, lbl

def collate_fn(batch):
    imgs, lbls = zip(*batch)
    max_w = max(i.shape[-1] for i in imgs)
    imgs = [F.pad(i, (0, max_w-i.shape[-1])) for i in imgs]
    imgs = torch.stack(imgs)
    lbl_lens = torch.tensor([len(l) for l in lbls], dtype=torch.int32)
    labels = torch.cat(lbls)
    return imgs, labels, lbl_lens

# train/val split
items = list(pseudo_labels.items())
random.shuffle(items); split = int(0.85*len(items))
train_ds = OCRDataset(dict(items[:split]))
val_ds   = OCRDataset(dict(items[split:]))
train_dl = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)
val_dl   = DataLoader(val_ds,   batch_size=8, shuffle=False, collate_fn=collate_fn)

# ==============================================================
# 10. CRNN-CTC –º–æ–¥–µ–ª—å
# ==============================================================
class CRNN(nn.Module):
    def __init__(self, cnn_out=256, num_classes=len(char2idx)+1):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2,2),
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2,2),
            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU())
        self.bi = nn.LSTM(256, 256, 2, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(512, num_classes)
    def forward(self, x):
        x = self.cnn(x)                    # B√ó256√óH/4√óW/4
        b,c,h,w = x.shape
        x = x.permute(0,3,1,2).contiguous()  # B√óT√óC√óH'
        x = x.flatten(3).mean(-1)          # B√óT√óC
        x,_ = self.bi(x)
        x = self.fc(x)
        return x.log_softmax(-1)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CRNN().to(device)
criterion = nn.CTCLoss(blank=0, zero_infinity=True)
opt = torch.optim.Adam(model.parameters(), lr=1e-4)

# ==============================================================
# 11. –û–±—É—á–µ–Ω–∏–µ
# ==============================================================
def train_epoch(dl):
    model.train(); tot=0
    for imgs, labels, lbl_lens in dl:
        T = model(imgs.to(device)).size(1)   # –¥–ª–∏–Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–π –æ—Å–∏
        assert (T >= lbl_lens).all(), f"T={T}, y={lbl_lens}"
        break
        imgs = imgs.to(device)
        out  = model(imgs)               # B√óT√óC
        T = out.size(1)
        logp = out.permute(1,0,2)
        inp_lens = torch.full((logp.size(1),), T, dtype=torch.int32)
        loss = criterion(logp, labels, inp_lens, lbl_lens)
        opt.zero_grad(); loss.backward(); opt.step()
        tot += loss.item()
    return tot/len(dl)

def val_epoch(dl):
    model.eval(); tot=0
    with torch.no_grad():
        for imgs, labels, lbl_lens in dl:
            imgs = imgs.to(device)
            out  = model(imgs)
            T = out.size(1)
            logp = out.permute(1,0,2)
            inp_lens = torch.full((logp.size(1),), T, dtype=torch.int32)
            loss = criterion(logp, labels, inp_lens, lbl_lens)
            tot += loss.item()
    return tot/len(dl)

for ep in range(20):
    tr = train_epoch(train_dl)
    vl = val_epoch(val_dl)
    print(f"Epoch {ep+1:02d}: train {tr:.3f} | val {vl:.3f}")

# ==============================================================
# 12. –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –Ω–∞ —Ä–∞–Ω–¥–æ–º–Ω–æ–º —Å–∫—Ä–∏–Ω—à–æ—Ç–µ
# ==============================================================
model.eval()
test_img = random.choice(list(DATA_DIR.glob('Screenshot_*.jpg')))
img = preprocess_pillow(Image.open(test_img), target_h=64)
with torch.no_grad():
    logits = model(transforms.ToTensor()(img).unsqueeze(0).to(device))
pred = logits.argmax(-1)[0].cpu().numpy()

# Greedy CTC decode
out, prev = [], 0
for idx in pred:
    if idx!=prev and idx!=0: out.append(idx)
    prev = idx
print("–ö–∞—Ä—Ç–∏–Ω–∫–∞:", test_img.name)
print("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:", decode_seq(out))

# ---------------------------------------------------------------
# 13. –≠–∫—Å–ø–æ—Ä—Ç ONNX + –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
# ---------------------------------------------------------------
dummy = torch.randn(1,1,64,512).to(device)   # —Ñ–∏–∫—Å–∏—Ä—É–µ–º –º–∞–∫—Å —à–∏—Ä–∏–Ω—É 512 px
torch.onnx.export(model, dummy, 'ocr_model.onnx', input_names=['image'], output_names=['logits'],
                  dynamic_axes={'image':{3:'width'}, 'logits':{1:'time'}})
print("ONNX saved!")

import onnxruntime as ort
sess = ort.InferenceSession('ocr_model.onnx',
                            providers=['CPUExecutionProvider'])
print("ONNX runtime loaded.")

# –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
!python -m onnxruntime.quantization.quantize_dynamic --model_path ocr_model.onnx --output_model ocr_model_int8.onnx --weight_type QInt8

# ---------------------------------------------------------------
# 14. –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–µ (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ)
# ---------------------------------------------------------------
def infer_onnx(path):
    img = preprocess_pillow(Image.open(path), target_h=64)
    w = img.size[0]
    pad_w = math.ceil(w/32)*32
    img = img.resize((pad_w,64), Image.BICUBIC)
    arr = np.array(img).astype(np.float32)/255.
    arr = (arr-0.5)/0.5
    arr = arr[None,None,:,:]
    logits = sess.run(None, {'image':arr})[0]  # (T,1,C)
    pred = logits.argmax(-1).squeeze(1)
    out=[]; prev=0
    for idx in pred:
        if idx!=prev and idx!=0:
            out.append(idx)
        prev=idx
    return decode_seq(out)

test_img = random.choice(list(DATA_DIR.glob('Screenshot_*.png')))
print("Test image:", test_img.name)
print("Inference:", infer_onnx(test_img))

# ---------------------------------------------------------------
# 15. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
# ---------------------------------------------------------------
print("""
‚úÖ –ú—ã –ø–æ—Å—Ç—Ä–æ–∏–ª–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –±–µ–∑—Ä–∞–∑–º–µ—Ç–æ—á–Ω—ã–π pipeline:
   1) ViT-MPLM self-supervised –æ–±—É—á–∞–µ—Ç—Å—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ patch-—Ç–æ–∫–µ–Ω—ã
   2) –°—ã—Ä—ã–µ OCR-–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Tesseract –æ—á–∏—â–∞—é—Ç—Å—è kenLM-–º–æ–¥–µ–ª—å—é
   3) CRNN-CTC –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø—Å–µ–≤–¥–æ-—Ä–∞–∑–º–µ—Ç–∫–µ; CER ‚Üì –¥–æ {:.3f}
   4) –ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ ONNX + Int8 quantization
""".format(best_cer))



# ==============================================================
# 8. –ß–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –º–µ—Ç–æ–∫ + –∞–ª—Ñ–∞–≤–∏—Ç
# ==============================================================

# —É–¥–∞–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≥–¥–µ Tesseract –Ω–∏—á–µ–≥–æ –Ω–µ —É–≤–∏–¥–µ–ª
pseudo_labels = {k:v for k,v in pseudo_labels.items() if v.strip()}
print(f'–ü–æ—Å–ª–µ —á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(pseudo_labels)} –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫')

ALL_CHARS = sorted(
    set(''.join(pseudo_labels.values()) +
        " .,:%()"+string.ascii_letters+string.digits)
)
char2idx = {c:i+1 for i,c in enumerate(ALL_CHARS)}   # 0=blank
idx2char = {i:c for c,i in char2idx.items()}

def encode_text(t): return [char2idx[c] for c in t if c in char2idx]
def decode_seq(seq): return ''.join(idx2char.get(i,'') for i in seq)

# ==============================================================
# 9. OCR-–¥–∞—Ç–∞—Å–µ—Ç –∏ collate_fn
# ==============================================================
class OCRDataset(Dataset):
    def __init__(self, labels, img_h=64):
        self.items = list(labels.items())
        self.img_h = img_h
        self.totensor = transforms.ToTensor()
    def __len__(self): return len(self.items)
    def __getitem__(self, idx):
        name, txt = self.items[idx]
        img = preprocess_pillow(Image.open(DATA_DIR/name), target_h=self.img_h)
        img = self.totensor(img)           # 1√óH√óW
        lbl = torch.tensor(encode_text(txt), dtype=torch.long)
        return img, lbl

def collate_fn(batch):
    imgs, lbls = zip(*batch)
    max_w = max(i.shape[-1] for i in imgs)
    imgs = [F.pad(i, (0, max_w-i.shape[-1])) for i in imgs]
    imgs = torch.stack(imgs)
    lbl_lens = torch.tensor([len(l) for l in lbls], dtype=torch.int32)
    labels = torch.cat(lbls)
    return imgs, labels, lbl_lens

# train/val split
items = list(pseudo_labels.items())
random.shuffle(items); split = int(0.85*len(items))
train_ds = OCRDataset(dict(items[:split]))
val_ds   = OCRDataset(dict(items[split:]))
train_dl = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)
val_dl   = DataLoader(val_ds,   batch_size=8, shuffle=False, collate_fn=collate_fn)

# ==============================================================
# 10. CRNN-CTC –º–æ–¥–µ–ª—å (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# ==============================================================
class CRNN(nn.Module):
    def __init__(self, cnn_out=256, num_classes=len(char2idx)+1):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2,2),
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2,2),
            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU())
        self.bi = nn.LSTM(256, 256, 2, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(512, num_classes)
    def forward(self, x):
        x = self.cnn(x)                    # B√ó256√óH/4√óW/4
        b,c,h,w = x.shape
        x = x.permute(0,3,1,2).contiguous()  # B√óT√óC√óH'
        x = x.flatten(3).mean(-1)          # B√óT√óC
        x,_ = self.bi(x)
        x = self.fc(x)
        return x.log_softmax(-1)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CRNN().to(device)
criterion = nn.CTCLoss(blank=0, zero_infinity=True)
opt = torch.optim.Adam(model.parameters(), lr=1e-4)

# ==============================================================
# 11. –û–±—É—á–µ–Ω–∏–µ
# ==============================================================
def train_epoch(dl):
    model.train(); tot=0
    for imgs, labels, lbl_lens in dl:
        imgs = imgs.to(device)
        out  = model(imgs)               # B√óT√óC
        T = out.size(1)
        logp = out.permute(1,0,2)
        inp_lens = torch.full((logp.size(1),), T, dtype=torch.int32)
        loss = criterion(logp, labels, inp_lens, lbl_lens)
        opt.zero_grad(); loss.backward(); opt.step()
        tot += loss.item()
    return tot/len(dl)

def val_epoch(dl):
    model.eval(); tot=0
    with torch.no_grad():
        for imgs, labels, lbl_lens in dl:
            imgs = imgs.to(device)
            out  = model(imgs)
            T = out.size(1)
            logp = out.permute(1,0,2)
            inp_lens = torch.full((logp.size(1),), T, dtype=torch.int32)
            loss = criterion(logp, labels, inp_lens, lbl_lens)
            tot += loss.item()
    return tot/len(dl)

for ep in range(20):
    tr = train_epoch(train_dl)
    vl = val_epoch(val_dl)
    print(f"Epoch {ep+1:02d}: train {tr:.3f} | val {vl:.3f}")

# ==============================================================
# 12. –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –Ω–∞ —Ä–∞–Ω–¥–æ–º–Ω–æ–º —Å–∫—Ä–∏–Ω—à–æ—Ç–µ
# ==============================================================
model.eval()
test_img = random.choice(list(DATA_DIR.glob('Screenshot_*.jpg')))
img = preprocess_pillow(Image.open(test_img), target_h=64)
with torch.no_grad():
    logits = model(transforms.ToTensor()(img).unsqueeze(0).to(device))
pred = logits.argmax(-1)[0].cpu().numpy()

# Greedy CTC decode
out, prev = [], 0
for idx in pred:
    if idx!=prev and idx!=0: out.append(idx)
    prev = idx
print("–ö–∞—Ä—Ç–∏–Ω–∫–∞:", test_img.name)
print("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:", decode_seq(out))

# ==============================================================
# 13. (–ù–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ) —ç–∫—Å–ø–æ—Ä—Ç ONNX
# ==============================================================

# dummy = torch.randn(1,1,64,512).to(device)
# torch.onnx.export(model, dummy, "ocr_model.onnx",
#                   input_names=["image"], output_names=["logits"],
#                   dynamic_axes={"image":{3:"width"}, "logits":{1:"time"}})
# print("ONNX —Å–æ—Ö—Ä–∞–Ω—ë–Ω ‚Üí ocr_model.onnx")



# ==============================================================
#  0. –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô
# ==============================================================
!apt-get update -qq
!apt-get install -y -qq tesseract-ocr libtesseract-dev
# —Ä—É—Å—Å–∫—É—é high-accuracy –º–æ–¥–µ–ª—å (~36 –ú–ë)
!wget -q https://github.com/tesseract-ocr/tessdata_best/raw/main/rus.traineddata \
       -O /usr/share/tesseract-ocr/4.00/tessdata/rus.traineddata

!pip install -q torch torchvision torchaudio==2.2.0--index-url https://download.pytorch.org/whl/cu118
!pip install -q albumentations==1.4.0 pillow pytesseract==0.3.10 editdistance \
               opencv-python-headless torchmetrics einops timm icecream

# ==============================================================
#  1. –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö
# ==============================================================
import os, random, re, string, math, json, zipfile, shutil, urllib.request, itertools, time
from pathlib import Path
from typing import List, Tuple, Dict

import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image, ImageOps

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torchvision.transforms import functional as TF

from einops import rearrange
import pytesseract
from torchmetrics.text import CharErrorRate
from icecream import ic

torch.manual_seed(17)
np.random.seed(17)
random.seed(17)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("‚úÖ Torch:", torch.__version__, "| Device:", device)

# ==============================================================
#  2. –ó–ê–ì–†–£–ó–ö–ê –î–ê–¢–ê–°–ï–¢–ê (–∞—Ä—Ö–∏–≤ + —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞)
# ==============================================================
DATA_URL = "https://www.dropbox.com/scl/fi/mmyjbuglk8iwvybw7qmf5/dataset.zip?rlkey=v10y9pqrqobbmy0tl0zm4l0jl&dl=1"
RAW_ZIP  = "/content/dataset.zip"
DATA_DIR = Path("/content/dataset")

if not DATA_DIR.exists():
    !wget -q "$DATA_URL" -O "$RAW_ZIP"
    with zipfile.ZipFile(RAW_ZIP) as zf:
        zf.extractall("/content/")
    # –∏–Ω–æ–≥–¥–∞ –∞—Ä—Ö–∏–≤ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–ª–æ–∂–µ–Ω–Ω—É—é –ø–∞–ø–∫—É ¬´dataset/dataset/...¬ª
    if not list(DATA_DIR.glob("Screenshot_*")):
        inner = next(DATA_DIR.glob("dataset"), None)
        if inner:
            for f in inner.iterdir(): shutil.move(str(f), DATA_DIR)
            inner.rmdir()
    print("üì¶ Dataset unpacked ‚Üí", DATA_DIR)

# ==============================================================
#  3. –í–ò–ó–£–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê 6 –°–õ–£–ß–ê–ô–ù–´–• –ö–ê–†–¢–ò–ù–û–ö
# ==============================================================
def show_samples(n=6):
    plt.figure(figsize=(15,3))
    for i,p in enumerate(random.sample(list(DATA_DIR.glob('Screenshot_*.*')), n),1):
        img = Image.open(p)
        plt.subplot(1,n,i); plt.imshow(img); plt.title(p.name); plt.axis('off')
    plt.show()
show_samples()

# ==============================================================
#  4. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–õ–Ø OCR (—Å–µ—Ä—ã–π ‚Üí CLAHE ‚Üí –∞–¥–∞–ø—Ç–∏–≤. –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è)
# ==============================================================
def preprocess_pillow(img: Image.Image, target_h:int=96) -> Image.Image:
    img = img.convert('L')
    arr = np.array(img)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    arr  = clahe.apply(arr)
    arr  = cv2.adaptiveThreshold(arr,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                 cv2.THRESH_BINARY, 31, 11)
    arr  = cv2.medianBlur(arr,3)
    img  = Image.fromarray(arr)
    w,h  = img.size
    img  = img.resize((int(w*target_h/h), target_h), Image.BICUBIC)
    return img

# ==============================================================
#  5. –ü–†–û–í–ï–†–ö–ê TESSERACT –ù–ê 25 –°–õ–£–ß–ê–ô–ù–´–• –§–ê–ô–õ–ê–•
# ==============================================================

TESS_CFG = "--oem 1 --psm 6"           # LSTM-only, ¬´–æ–¥–∏–Ω –±–ª–æ–∫ —Ç–µ–∫—Å—Ç–∞¬ª
def clean(txt:str) -> str:
    txt = re.sub(r'[^0-9A-Za-z–ê-–Ø–∞-—è.,:%() ]+', ' ', txt)
    return re.sub(r'\s{2,}', ' ', txt).strip()

sample_paths = random.sample(list(DATA_DIR.glob("Screenshot_*.*")), 25)
lens = []
for p in sample_paths:
    txt = pytesseract.image_to_string(preprocess_pillow(Image.open(p)),
                                      lang="rus", config=TESS_CFG)
    txt = clean(txt)
    lens.append(len(txt))
    print(f"{p.name:>15} | {txt[:60]}")
print("–°—Ä. –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Tesseract:", np.mean(lens))

# –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è sanity-–ø—Ä–æ–≤–µ—Ä–∫–∞: > 0.5 —Å–∏–º–≤–æ–ª–∞ –≤ —Å—Ä–µ–¥–Ω–µ–º = –û–ö
assert np.mean(lens) > 0.5, "Tesseract –Ω–∏—á–µ–≥–æ –Ω–µ –≤–∏–¥–∏—Ç ‚Äì –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —à—Ä–∏—Ñ—Ç/—è–∑—ã–∫!"

# ==============================================================
#  6. –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–°–ï–í–î–û-–ú–ï–¢–û–ö –î–õ–Ø –í–°–ï–• 200 –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô
# ==============================================================
pseudo = {}
for p in DATA_DIR.glob("Screenshot_*.*"):
    txt = pytesseract.image_to_string(preprocess_pillow(Image.open(p)),
                                      lang="rus", config=TESS_CFG)
    txt = clean(txt)
    if txt: pseudo[p.name] = txt
print(f"üëì  {len(pseudo)}/{len(list(DATA_DIR.glob('Screenshot_*.*')))} "
      "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–ª—É—á–∏–ª–∏ –Ω–µ–ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")

# ==============================================================
#  7. –ê–õ–§–ê–í–ò–¢ + –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ==============================================================
EXTRA = "#/-+"
ALL_CHARS = sorted(set(''.join(pseudo.values()) + EXTRA +
                       string.ascii_letters + string.digits + ' .,:%()'))
char2idx = {c:i+1 for i,c in enumerate(ALL_CHARS)}    # 0 = blank
idx2char = {i:c for c,i in char2idx.items()}

def enc(t): return [char2idx[c] for c in t if c in char2idx]
def dec(seq): return ''.join(idx2char.get(i,'') for i in seq)

# ==============================================================
#  8. DATASET / DATALOADER
# ==============================================================
class OCRDataset(Dataset):
    def __init__(self, labels:Dict[str,str], img_h:int=96):
        self.items = list(labels.items())
        self.img_h = img_h
        self.tt = transforms.ToTensor()
    def __len__(self): return len(self.items)
    def __getitem__(self, idx):
        name, txt = self.items[idx]
        img = preprocess_pillow(Image.open(DATA_DIR/name), self.img_h)
        return self.tt(img), torch.tensor(enc(txt), dtype=torch.long)

def ctc_collate(batch):
    # –≤—ã–±—Ä–æ—Å–∏–º –ø—Ä–∏–º–µ—Ä—ã, –≥–¥–µ —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω–µ–µ, —á–µ–º —Ö—Ä–æ–Ω–æ—Å—å (T)
    goods = []
    for img,lbl in batch:
        T = img.shape[-1]//4           # –ø–æ—Å–ª–µ –¥–≤—É—Ö MaxPool2d(2)
        if len(lbl) <= T: goods.append((img,lbl))
    if not goods: return None          # –±—Ä–æ—Å–∏–º –≤–µ—Å—å –±–∞—Ç—á
    imgs,lbls = zip(*goods)
    max_w = max(i.shape[-1] for i in imgs)
    imgs = [F.pad(i, (0,max_w-i.shape[-1])) for i in imgs]
    imgs = torch.stack(imgs)
    lbl_lens = torch.tensor([len(l) for l in lbls], dtype=torch.int32)
    labels   = torch.cat(lbls)
    return imgs, labels, lbl_lens

items = list(pseudo.items()); random.shuffle(items)
split = int(0.85*len(items))
train_ds = OCRDataset(dict(items[:split]))
val_ds   = OCRDataset(dict(items[split:]))
train_dl = DataLoader(train_ds, 8, True, collate_fn=ctc_collate)
val_dl   = DataLoader(val_ds,   8, False, collate_fn=ctc_collate)

# ==============================================================
#  9. CRNN-CTC –ú–û–î–ï–õ–¨ (–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è, –Ω–æ —É—Å—Ç–æ–π—á–∏–≤–∞—è)
# ==============================================================
class CRNN(nn.Module):
    def __init__(self, n_cls=len(char2idx)+1):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1,64,3,1,1), nn.ReLU(), nn.MaxPool2d(2,2),
            nn.Conv2d(64,128,3,1,1), nn.ReLU(), nn.MaxPool2d(2,2),
            nn.Conv2d(128,256,3,1,1), nn.ReLU())
        self.rnn = nn.LSTM(256,256,2,bidirectional=True,batch_first=True)
        self.fc  = nn.Linear(512,n_cls)
    def forward(self,x):
        x = self.cnn(x)                       # B√ó256√óH/4√óW/4
        b,c,h,w = x.shape
        x = x.permute(0,3,1,2).reshape(b,w, c*h//1)   # B√óT√óC
        x,_ = self.rnn(x)
        return self.fc(x).log_softmax(-1)

model = CRNN().to(device)
crit  = nn.CTCLoss(blank=0, zero_infinity=True)
opt   = torch.optim.Adam(model.parameters(), lr=1e-4)

# ==============================================================
# 10. –û–ë–£–ß–ï–ù–ò–ï (60 —ç–ø–æ—Ö, –≤—ã–≤–æ–¥ CER –∫–∞–∂–¥—ã–µ 5)
# ==============================================================
def loop_epoch(dl, train=True):
    model.train() if train else model.eval()
    tot=0; metric = CharErrorRate()
    for batch in dl:
        if batch is None: continue
        imgs,lbls,lbl_lens = batch
        imgs = imgs.to(device)
        out  = model(imgs)     # B√óT√óC
        T    = torch.full((out.size(0),), out.size(1), dtype=torch.int32)
        loss = crit(out.permute(1,0,2), lbls, T, lbl_lens)
        if train:
            opt.zero_grad(); loss.backward(); opt.step()
        tot += loss.item()
        # CER
        pred = out.argmax(-1).cpu().numpy()
        gt_texts=[]; pr_texts=[]
        idx=0
        for i,l in enumerate(lbl_lens):
            gt_texts.append(dec(lbls[idx:idx+l].tolist()))
            idx+=l
            seq,prev = [],0
            for v in pred[i]:
                if v!=prev and v!=0: seq.append(v)
                prev=v
            pr_texts.append(dec(seq))
        metric.update(pr_texts, gt_texts)
    return tot/max(1,len(dl)), metric.compute().item()

for ep in range(1,61):
    tr, _   = loop_epoch(train_dl, True)
    if ep%5==0:
        vl, cer = loop_epoch(val_dl, False)
        print(f"Epoch {ep:02d} | train {tr:.3f} | val {vl:.3f} | CER {cer:.2%}")

# ==============================================================
# 11. –¢–ï–°–¢: –≤—ã–≤–æ–¥ 5 –ø—Ä–∏–º–µ—Ä–æ–≤
# ==============================================================
model.eval()
plt.figure(figsize=(12,8))
for i,p in enumerate(random.sample(list(val_ds.items), 5),1):
    img = preprocess_pillow(Image.open(DATA_DIR/p[0]),96)
    with torch.no_grad():
        logits = model(transforms.ToTensor()(img).unsqueeze(0).to(device))
    seq,prev=[],0
    for v in logits.argmax(-1)[0].cpu().numpy():
        if v!=prev and v!=0: seq.append(v)
        prev=v
    txt = dec(seq)
    plt.subplot(5,2,2*i-1); plt.imshow(img,cmap='gray'); plt.axis('off'); plt.title(p[0])
    plt.subplot(5,2,2*i);   plt.text(0.01,0.5,txt,fontsize=10); plt.axis('off')
plt.show()

# ==============================================================
# 12. (–û–ü–¶–ò–û–ù–ê–õ–¨–ù–û) –≠–ö–°–ü–û–†–¢ ONNX
# ==============================================================
# dummy = torch.randn(1,1,96,512).to(device)
# torch.onnx.export(model, dummy, "ocr_model.onnx",
#                   input_names=["image"], output_names=["logits"],
#                   dynamic_axes={"image":{3:"width"}, "logits":{1:"time"}})
# print("üì§ onnx-—Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: ocr_model.onnx")



# ==============================================================
# 0.  –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
# ==============================================================
!apt-get update -qq
!apt-get install -y -qq tesseract-ocr libtesseract-dev

!pip install -q openai pillow numpy opencv-python-headless python-Levenshtein \
                tqdm matplotlib

import os, random, json, base64, re, zipfile, io, textwrap, time
from pathlib import Path
from typing import List, Dict

import numpy as np
import cv2
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
import pytesseract
import Levenshtein as Lev
import openai

# ----------  –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ Proxy API ----------
PROXY_BASE = "https://api.proxyapi.ru/openai/v1"
OPENAI_API_KEY = os.getenv("PROXY_API_KEY")  #  <<< —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º!
assert OPENAI_API_KEY, "export PROXY_API_KEY=<–≤–∞—à_–∫–ª—é—á>"

client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url=PROXY_BASE)

def gpt4o(messages, model="gpt-4o-mini", max_tokens=400, T=0.0):
    return client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=T,
        max_tokens=max_tokens
    ).choices[0].message.content.strip()

# ---------- —Å–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç ----------
DATA_URL = "https://www.dropbox.com/scl/fi/mmyjbuglk8iwvybw7qmf5/dataset.zip?dl=1"
ZIP = "dataset.zip"; DATA = Path("dataset")
if not DATA.exists():
    import urllib.request, tempfile, shutil
    urllib.request.urlretrieve(DATA_URL, ZIP)
    with zipfile.ZipFile(ZIP) as zf: zf.extractall()
    # –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å, –µ—Å–ª–∏ –µ—Å—Ç—å
    inner = list(DATA.glob("dataset"))
    if inner:
        for f in inner[0].iterdir(): shutil.move(str(f), DATA)
        inner[0].rmdir()
print("–§–∞–π–ª–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:", len(list(DATA.glob("Screenshot_*"))))

# ==============================================================
# 1.  –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ==============================================================
def to_jpeg_bytes(pil_img:Image.Image, max_side=768, quality=30)->bytes:
    w,h = pil_img.size
    if max(w,h) > max_side:
        scale = max_side/max(w,h)
        pil_img = pil_img.resize((int(w*scale), int(h*scale)), Image.BICUBIC)
    buf = io.BytesIO()
    pil_img.save(buf, format="JPEG", quality=quality)
    return buf.getvalue()

PSM = "--oem 1 --psm 6"

def tesseract_ocr(pil_img:Image.Image)->str:
    txt = pytesseract.image_to_string(pil_img, lang="rus", config=PSM)
    txt = re.sub(r'[^0-9A-Za-z–ê-–Ø–∞-—è.,:%() ]+', ' ', txt)
    return re.sub(r'\s{2,}', ' ', txt).strip()

def vision_correct(image_b64:str, raw_text:str)->str:
    system = "–¢—ã OCR-—ç–∫—Å–ø–µ—Ä—Ç. –ò—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ, —Å–≤–µ—Ä—è—è—Å—å —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º. –í–µ—Ä–Ω–∏ –û–î–ù–£ —Å—Ç—Ä–æ–∫—É."
    user = [
        {"type":"input_text","text":f"–ß–µ—Ä–Ω–æ–≤–æ–π OCR: ¬´{raw_text}¬ª. –ò—Å–ø—Ä–∞–≤—å —Ç–∞–∫, —á—Ç–æ–±—ã –±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã —Å–æ–≤–ø–∞–¥–∞–ª–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º."},
        {"type":"input_image","image_url":f"data:image/jpeg;base64,{image_b64}"}
    ]
    msg=[{"role":"system","content":system},
         {"role":"user","content":user}]
    return gpt4o(msg, max_tokens=200)

def rule_mining(raw:str, fixed:str)->List[str]:
    system = ("–°—Ä–∞–≤–Ω–∏ –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ OCR –∏ –≤—ã–≤–µ–¥–∏ –¥–æ 5 –ø—Ä–∞–≤–∏–ª –≤–∏–¥–∞ "
              "`s/–æ—à–∏–±–∫–∞/–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ/` –∏–ª–∏ `delete(—Å–∏–º–≤–æ–ª)`.")
    user = f"RAW: {raw}\nFIXED: {fixed}"
    msg=[{"role":"system","content":system},
         {"role":"user","content":user}]
    rules = gpt4o(msg, max_tokens=150, T=0.2)
    return [r.strip() for r in rules.splitlines() if r.strip()]

def apply_rules(text:str, rules:List[str])->str:
    for r in rules:
        if r.startswith("s/"):
            _,bad,good,_ = r.split("/",3)
            text = text.replace(bad,good)
        elif r.startswith("delete("):
            ch = r[7:-1]
            text = text.replace(ch,"")
    return text

def cer(a:str,b:str)->float:
    return Lev.distance(a,b)/max(1,len(b))

# ==============================================================
# 2.  –û—Ç–±–∏—Ä–∞–µ–º 15 —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
# ==============================================================
all_imgs = sorted(DATA.glob("Screenshot_*"))
SAMPLE = random.sample(all_imgs, 15)

# ==============================================================
# 3.  –ü—Ä–æ—Ö–æ–¥–∏–º pipeline ‚Üí —Å–æ–±–∏—Ä–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞
# ==============================================================
results=[]; global_rules=[]
for p in tqdm(SAMPLE, desc="Processing"):
    pil = Image.open(p).convert("RGB")
    raw = tesseract_ocr(pil)
    jpeg_bytes = to_jpeg_bytes(pil)
    img64 = base64.b64encode(jpeg_bytes).decode()
    fixed = vision_correct(img64, raw)

    # –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
    rules = rule_mining(raw, fixed)
    global_rules.extend(rules)

    results.append({
        "file":p.name,
        "raw":raw,
        "fixed":fixed,
        "rules":rules,
        "cer_raw":cer(raw,fixed),
        "cer_fixed":0.0
    })

# deduplicate rules
global_rules = list(dict.fromkeys(global_rules))
print("üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø—Ä–∞–≤–∏–ª:", len(global_rules))
print("\n".join(global_rules[:10]))

# ==============================================================
# 4.  –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –Ω–∞ –Ω–æ–≤–æ–π –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ
# ==============================================================
TEST = [x for x in all_imgs if x not in SAMPLE][:10]
before=[]; after=[]
for p in TEST:
    pil = Image.open(p).convert("RGB")
    raw = tesseract_ocr(pil)
    corrected = apply_rules(raw, global_rules)
    before.append(cer(raw, corrected))     # —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ raw vs corrected
    # —Å—á–∏—Ç–∞–µ–º oracle CER —á–µ—Ä–µ–∑ GPT-–∫–æ—Ä—Ä–µ–∫—Ü–∏—é
    fixed = vision_correct(
        base64.b64encode(to_jpeg_bytes(pil)).decode(), raw)
    after.append(cer(corrected, fixed))

print(f"\n–î–æ –ø—Ä–∞–≤–∏–ª  CER‚âà{np.mean(before):.2f} ‚îÇ –ø–æ—Å–ª–µ –ø—Ä–∞–≤–∏–ª  CER‚âà{np.mean(after):.2f}")

# ==============================================================
# 5.  –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º 3 –ø—Ä–∏–º–µ—Ä–∞
# ==============================================================
plt.figure(figsize=(12,6))
for i,(p,bef) in enumerate(zip(TEST[:3], before),1):
    pil = Image.open(p).convert("RGB")
    raw = tesseract_ocr(pil)
    rule_txt = apply_rules(raw, global_rules)
    plt.subplot(3,2,2*i-1); plt.imshow(pil); plt.axis('off'); plt.title(p.name)
    txt = textwrap.fill(f"RAW: {raw}\nRULE‚Üí {rule_txt}", 50)
    plt.subplot(3,2,2*i); plt.text(0,0.5,txt,fontsize=9); plt.axis('off')
plt.tight_layout(); plt.show()

# ==============================================================
# 6.  –°–æ—Ö—Ä–∞–Ω—è–µ–º ¬´—á–∏—Å—Ç—É—é¬ª —Ä–∞–∑–º–µ—Ç–∫—É –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
# ==============================================================
clean_pairs = {r["file"]: r["fixed"] for r in results}
json.dump(clean_pairs, open("clean_labels.json","w",encoding="utf8"), ensure_ascii=False, indent=2)
print("üíæ clean_labels.json —Å–æ—Ö—Ä–∞–Ω—ë–Ω.")
